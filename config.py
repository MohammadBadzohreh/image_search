# config.py
import torch

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ ---
MILVUS_URI = "http://milvus-standalone:19530" 
IMAGE_STORAGE_PATH = "/home/jovyan/work/benchmark/data/flickr30k/Images"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

CAPTION_MODEL = "Salesforce/blip-image-captioning-base" 

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ ---
MODELS_CONFIG = {
    # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ (SigLIP Ùˆ Jina) Ø³Ø± Ø¬Ø§ÛŒØ´Ø§Ù† Ù‡Ø³ØªÙ†Ø¯
    "SigLIP": {
        "model_id": "google/siglip-so400m-patch14-384",
        "collection_name": "siglip_gallery_v3_captioned",
        "dimension": 1152,
        "type": "siglip"
    },
    "Jina CLIP v1": {
        "model_id": "jinaai/jina-clip-v1", 
        "collection_name": "jina_clip_v1_embedding",
        "dimension": 768,
        "type": "jina"
    },
    "Jina CLIP v2": {
        "model_id": "jinaai/jina-clip-v2", 
        "collection_name": "jina_clip_v2_embedding",
        "dimension": 1024,
        "type": "jina"
    },
    # ðŸ‘‡ Ù…Ø¯Ù„ Ø¬Ø¯ÛŒØ¯ CLIPA-v2 (Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù„ÛŒÙ†Ú© Ø´Ù…Ø§)
    "CLIPA-v2 (ViT-H-14)": {
        # Ù¾ÛŒØ´ÙˆÙ†Ø¯ hf-hub: ÛŒØ¹Ù†ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ù„ÛŒÙ†Ú© Ø´Ù…Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´ÙˆØ¯
        "model_id": "hf-hub:UCSC-VLAA/ViT-H-14-CLIPA-336-laion2B", 
        "collection_name": "clipa_v2_h14_336",
        "dimension": 1024, # Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø¯Ù„ ViT-H Ù‡Ù…ÛŒØ´Ù‡ 1024 Ø§Ø³Øª
        "type": "open_clip" # ðŸ‘ˆ Ù†ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¯Ù‚ÛŒÙ‚
    }
}