% Compile with XeLaTeX
\documentclass[12pt]{article}

% بسته‌های ریاضی، لینک، گرافیک
\usepackage{amsmath, amssymb, amsfonts, mathtools}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{longtable}

% --- تغییر شماره ۱: اضافه کردن بسته float برای مدیریت دقیق جایگاه تصاویر ---
\usepackage{float}

% ------------ xepersian همیشه آخر -------------
\usepackage{xepersian}

% تنظیم فونت IranSansX
\settextfont[
BoldFont={IRANSansX Bold},
ItalicFont={IRANSansX Medium},
BoldItalicFont={IRANSansX ExtraBold}
]{IRANSansX Light}

\setlatintextfont{Times New Roman}

% حروف‌چینی بهتر
\setlength{\emergencystretch}{3em}
\linespread{1.25}

\title{گزارش پیاده‌سازی سامانه جستجوی معنایی تصاویر}



\title{
	\textbf{گزارش سه ماهه پروژه کسری پروژه} \\
	\vspace{0.5cm} % فاصله بین عنوان اصلی و نام پروژه
	ساختاربخشی به داده‌های غیرساختارمند تصویری و ارائه سامانه جستجو مبتنی بر تصاویر
	
	\vspace{0.2cm}
	\textbf{نام شرکت/سازمان:} \\
	\Large
	شرکت ژرفا تک
}

\author{سید محمد بادزهره}
\date{16 آذر 1404}

\begin{document}
	\maketitle
	
	\begin{abstract}
		در این گزارش، سامانه‌ای برای جستجوی معنایی تصاویر با استفاده از مدل‌های پیشرفته یادگیری عمیق معرفی شده است. این سامانه بر پایه استخراج بردارهای معنایی از تصاویر، ذخیره‌سازی آن‌ها در پایگاه‌داده برداری \lr{Milvus} و انجام جستجوی چندوجهی مبتنی بر مدل‌های ترنسفورمری مانند \lr{SigLIP} و همچنین تولید کپشن خودکار با مدل \lr{BLIP} طراحی شده است. هدف این سیستم فراهم‌کردن بستری دقیق، مقیاس‌پذیر، سریع و قابل‌اتکا برای جستجوی محتوای تصویری است.
	\end{abstract}
	
	% ------------------------------------
	\section*{۱. مقدمه}
	% ------------------------------------
	
	\subsection*{۱-۱ هوشمندی \lr{(Accuracy \& Intelligence)}}
	در این سامانه برای استخراج ویژگی‌های معنایی از تصاویر از مدل \lr{SigLIP} استفاده شده است. این مدل نسخه‌ای پیشرفته‌تر از \lr{CLIP} است و با حذف وابستگی به \lr{Softmax Contrastive Loss} و جایگزینی آن با زیان سیگموید باینری، توانسته دقت هم‌ترازی متن–تصویر را به‌طور چشمگیری افزایش دهد.
	مدل \lr{SigLIP} به‌جای مجبورکردن سیستم به انتخاب تنها یک جفت صحیح، به مدل اجازه می‌دهد چندین متن یا تصویر را به‌صورت مستقل ارزیابی کرده و احتمال وابستگی هر جفت را به‌طور جداگانه محاسبه کند. این تغییر باعث می‌شود مدل نسبت به نویز مقاوم‌تر بوده و در داده‌های دنیای واقعی عملکرد بسیار بهتری داشته باشد.
	خروجی این مدل یک بردار معنایی با ابعاد بالا است که تصویر را در فضای embedding قرار می‌دهد. این فضای مشترک، مبنای اصلی جستجوی معنایی در پروژه است.
	
	\subsection*{۱-۲ قابلیت جستجوی چندگانه \lr{(Multimodal)}}
	این سامانه از دو نوع جستجو پشتیبانی می‌کند:
	\begin{itemize}
		\item \lr{Text-to-Image}: کاربر جمله یا عبارت متنی را وارد می‌کند و سیستم تصاویر مرتبط را بر اساس شباهت برداری پیشنهاد می‌دهد.
		\item \lr{Image-to-Image}: یک تصویر ورودی گرفته شده و مشابه‌ترین تصاویر موجود در پایگاه داده نمایش داده می‌شود.
	\end{itemize}
	توانایی مدل در فهم هم‌زمان زبان و تصویر باعث شده این جستجوها دقت بالایی داشته باشند و نتایج بسیار مرتبطی نمایش داده شود.
	
	\subsection*{۱-۳ تولید توضیحات خودکار \lr{(Auto-Captioning)}}
	برای تولید توضیحات خودکار از مدل \lr{BLIP} استفاده شده است.
	\lr{BLIP} (مخفف \lr{Bootstrapping Language-Image Pre-training}) یک مدل چندوجهی بسیار پیشرفته است که می‌تواند محتوای تصاویر را به شکل توصیفی و دقیق بیان کند.
	
	این مدل شامل دو بخش کلیدی است:
	\begin{itemize}
		\item \textbf{Captioning Model}: تولید جملات دقیق، طبیعی و روان درباره محتوای تصویر
		\item \textbf{Image-Text Matching}: تشخیص میزان ارتباط تصویر با یک متن خاص
	\end{itemize}
	
	\lr{BLIP} از تکنیک \lr{Bootstrapping} استفاده می‌کند؛ یعنی خود مدل داده‌های نویزی را تشخیص می‌دهد، داده‌های بهتر تولید می‌کند و کیفیت یادگیری را بالا می‌برد.
	به‌کارگیری این مدل در سیستم باعث شده عملیات برچسب‌گذاری خودکار، جستجوی معنایی و بازیابی دقیق‌تر تصاویر بهبود قابل‌توجهی داشته باشد.
	
	% ------------------------------------
	\section*{۲. قابلیت‌های پیشرفته کاربری}
	% ------------------------------------
	
	\subsection*{۲-۱ جستجوی ناحیه‌ای \lr{(Crop \& Search)}}
	این قابلیت به کاربر اجازه می‌دهد بخشی از تصویر را انتخاب کرده و سیستم تنها همان بخش را تحلیل کند.
	نواحی کوچک مانند یک «شیء»، «برچسب»، «چهره» یا «لوگو» را می‌توان با دقت بسیار بالا جستجو کرد. این قابلیت با کمک مکانیزم‌های استخراج ویژگی محلی انجام می‌شود.
	
	\subsection*{۲-۲ جستجوی ترکیبی \lr{(Hybrid Search)}}
	این سیستم علاوه‌بر جستجوی معنایی، از جستجوی ترکیبی نیز پشتیبانی می‌کند. در این روش، نتایج براساس ترکیبی از فیلترها محدود می‌شوند:
	\begin{itemize}
		\item نام فایل \lr{(Filename)}
		\item تاریخ بارگذاری \lr{(Upload Date)}
		\item برچسب‌ها \lr{(Tags)}
	\end{itemize}
	این ترکیب باعث می‌شود جستجو علاوه بر دقت معنایی، دقت ساختاری نیز داشته باشد.
	
	\subsection*{۲-۳ پاک‌سازی داده‌ها \lr{(Deduplication)}}
	یکی از مشکلات رایج در دیتاست‌های تصویری، ذخیره تصاویر تکراری است.
	در این پروژه با استفاده از فاصله برداری میان embeddings و آستانه‌های مشابهت، تصاویر تکراری شناسایی و حذف شده‌اند.
	این کار باعث:
	\begin{itemize}
		\item کاهش مصرف فضای ذخیره‌سازی
		\item افزایش کیفیت نتایج جستجو
		\item کاهش بار سیستم
	\end{itemize}
	
	% ------------------------------------
	\section*{۳. ابزارها و تکنولوژی‌های مورد استفاده}
	% ------------------------------------
	
	\subsection*{۳-۱ دیتابیس برداری \lr{Milvus}}
	\lr{Milvus} یک پایگاه‌داده برداری \lr{Cloud-Native} و متن‌باز است که به‌طور خاص برای مدیریت بردارهای \lr{Embedding} طراحی شده است.
	این پایگاه‌داده برخلاف دیتابیس‌های رابطه‌ای (مانند \lr{PostgreSQL}) برای داده‌های با ابعاد بالا بهینه شده است و می‌تواند میلیون‌ها بردار را با سرعت بالا ذخیره و بازیابی کند.
	
	\textbf{ویژگی‌های کلیدی Milvus:}
	\begin{itemize}
		\item پشتیبانی از شاخص‌های پیشرفته \lr{ANN} مانند \lr{HNSW} و \lr{IVF\_PQ}
		\item مقیاس‌پذیری افقی با استفاده از \lr{Query Node}، \lr{Data Node} و \lr{Index Node}
		\item امکان اجرا روی \lr{Docker} و \lr{Kubernetes}
		\item سرعت جستجوی میلی‌ثانیه‌ای
	\end{itemize}
	
	\textbf{معماری Milvus} به گونه‌ای طراحی شده که بار محاسباتی را بین چندین نود توزیع کند، و همین موضوع آن را برای پردازش دیتاست‌های بزرگ تصویری ایده‌آل می‌کند.
	
	\subsection*{۳-۲ پلتفرم کانتینری \lr{Docker}}
	برای تضمین قابلیت حمل، نصب آسان و جلوگیری از مشکلات ناسازگاری کتابخانه‌ها، تمامی بخش‌های سیستم در قالب کانتینر اجرا شده‌اند.
	با استفاده از \lr{docker-compose} چندین سرویس شامل:
	\begin{itemize}
		\item پایگاه داده Milvus
		\item موتور هوش مصنوعی
		\item رابط کاربری
	\end{itemize}
	در محیطی ایزوله اجرا می‌شوند.
	
	\subsection*{۳-۳ فریم‌ورک رابط کاربری \lr{Streamlit}}
	\lr{Streamlit} یک ابزار بسیار مناسب برای توسعه سریع رابط‌های مبتنی بر پایتون است.
	این فریم‌ورک بدون نیاز به \lr{HTML/CSS/JS} اجازه می‌دهد تنها با چند خط کد، یک پنل کامل جستجو بسازیم.
	کاربرد اصلی آن در پروژه:
	\begin{itemize}
		\item بارگذاری و نمایش تصویر
		\item کراپ ناحیه دلخواه
		\item نمایش نتایج جستجو
	\end{itemize}
	
	\subsection*{۳-۴ کتابخانه‌های \lr{PyTorch \& Transformers}}
	هسته یادگیری عمیق این سامانه با استفاده از کتابخانه‌های پرقدرت:
	\begin{itemize}
		\item \lr{PyTorch}
		\item \lr{Hugging Face Transformers}
	\end{itemize}
	طراحی شده است.
	
	این کتابخانه‌ها امکان لود مدل‌های پیش‌آموزش‌دیده مانند:
	\begin{itemize}
		\item \lr{SigLIP}
		\item \lr{BLIP}
	\end{itemize}
	را فراهم کرده‌اند. این مدل‌ها به‌صورت مستقیم برای استخراج ویژگی و تولید کپشن در سیستم استفاده شده‌اند.
	
	\vspace{2em}
	\hrule
	\vspace{1em}
	
	% ------------------------------------
	\section*{۲. مبانی نظری و پیشینه پژوهش}
	% ------------------------------------
	
	\subsection*{۲-۱ مقدمه}
	در این فصل به بررسی مفاهیم بنیادین و مبانی نظری مورد استفاده در این پژوهش می‌پردازیم. با توجه به ماهیت چندرشته‌ای \lr{(Multidisciplinary)} پروژه، مفاهیم در سه حوزه اصلی دسته‌بندی می‌شوند: هوش مصنوعی و یادگیری عمیق (با تمرکز بر مدل‌های چندوجهی)، پایگاه‌های داده برداری (با تمرکز بر الگوریتم‌های جستجو) و مهندسی نرم‌افزار (معماری کانتینری). درک عمیق این مفاهیم برای توجیه انتخاب‌های فنی صورت گرفته در فصول بعدی ضروری است.
	
	\subsection*{۲-۲ نمایش برداری داده‌ها \lr{(Vector Representation)}}
	یکی از چالش‌های اصلی در علوم کامپیوتر، نحوه تفهیم داده‌های غیرساختاریافته \lr{(Unstructured Data)} مانند تصویر، صدا و متن به ماشین است. کامپیوترها تنها قادر به پردازش اعداد هستند. فرآیند تبدیل داده‌ها به بردارهای عددی، تعبیه برداری \lr{(Vector Embedding)} نامیده می‌شود.
	
	\subsubsection*{۲-۲-۱ مفهوم فضای معنایی \lr{(Semantic Space)}}
	در روش‌های سنتی (مانند \lr{One-Hot Encoding})، کلمات یا تصاویر به صورت مستقل و بدون ارتباط با یکدیگر کدگذاری می‌شدند. اما در روش \lr{Embedding}، هدف نگاشت داده‌ها به یک فضای برداری با ابعاد بالا \lr{(High-dimensional Vector Space)} است؛ به گونه‌ای که مفاهیم مشابه از نظر معنایی، در این فضا از نظر هندسی به یکدیگر نزدیک باشند.
	
	برای مثال، اگر بردار کلمه «پادشاه» را $V_{king}$ و بردار «مرد» را $V_{man}$ بنامیم، در یک فضای معنایی ایده‌آل رابطه زیر برقرار است:
	$$V_{king} - V_{man} + V_{woman} \approx V_{queen}$$
	این مفهوم در تصاویر نیز صادق است. بردار تصویر یک «گربه» باید به بردار تصویر یک «ببر» نزدیک‌تر باشد تا به بردار تصویر یک «خودرو».
	
	\subsubsection*{۲-۲-۲ شکاف معنایی \lr{(The Semantic Gap)}}
	شکاف معنایی به تفاوت بین «اطلاعات سطح پایین» (پیکسل‌ها در تصویر) و «اطلاعات سطح بالا» (مفهوم و تفسیر تصویر توسط انسان) اطلاق می‌شود. مدل‌های یادگیری عمیق با استخراج ویژگی‌ها \lr{(Feature Extraction)} در لایه‌های مختلف، سعی در پر کردن این شکاف دارند.
	
	\subsection*{۲-۳ معماری ترنسفورمرها \lr{(Transformers)}}
	انقلاب اصلی در حوزه هوش مصنوعی با معرفی معماری \lr{Transformer} توسط گوگل در سال ۲۰۱۷ (مقاله \lr{"Attention is All You Need"}) آغاز شد. اگرچه این معماری ابتدا برای ترجمه ماشینی \lr{(NLP)} طراحی شد، اما به سرعت جایگزین شبکه‌های عصبی کانولوشنی \lr{(CNN)} در پردازش تصویر گردید.
	
	
	% --- تغییر شماره ۲: استفاده از [H] برای قفل کردن جایگاه تصویر ---
	\begin{figure}[H]
		\centering
		% فایل تصویر خود را با نام transformer_arch.png ذخیره کنید و خط زیر را از حالت کامنت خارج کنید
		\includegraphics[width=0.8\textwidth]{images/transformers.png}
		
		\caption{معماری کلی مدل ترنسفورمر}
	\end{figure}
	
	\subsubsection*{۲-۳-۱ مکانیزم توجه \lr{(Self-Attention Mechanism)}}
	قلب تپنده ترنسفورمرها، مکانیزم «توجه خودکار» است. این مکانیزم به مدل اجازه می‌دهد تا وزن‌دهی پویایی به بخش‌های مختلف داده ورودی داشته باشد. فرمول ریاضی توجه به صورت زیر تعریف می‌شود:
	$$Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
	که در آن:
	\begin{itemize}
		\item \textbf{Q (Query)}: پرس‌وجو (آنچه به دنبالش هستیم).
		\item \textbf{K (Key)}: کلید (ویژگی‌هایی که داریم).
		\item \textbf{V (Value)}: مقدار (محتوای اصلی).
		\item \textbf{$d_k$}: ابعاد بردار کلید (جهت نرمال‌سازی).
	\end{itemize}
	این مکانیزم باعث می‌شود مدل بتواند وابستگی‌های دوربرد \lr{(Long-range dependencies)} را در داده‌ها درک کند، چیزی که شبکه‌های \lr{CNN} در انجام آن محدودیت داشتند.
	
	\subsection*{۲-۴ ترنسفورمرهای بینایی \lr{(Vision Transformers - ViT)}}
	موفقیت ترنسفورمرها در متن، محققان را بر آن داشت تا این معماری را به تصاویر تعمیم دهند. مدل \lr{ViT} تصویر ورودی را به قطعات کوچک مربعی \lr{(Patch)} تقسیم می‌کند (مثلاً ۱۶×۱۶ پیکسل).
	
	هر پچ به یک بردار خطی تبدیل شده \lr{(Linear Projection)} و سپس یک «تعبیه مکانی» \lr{(Positional Embedding)} به آن اضافه می‌شود تا مدل بداند هر تکه مربوط به کجای تصویر است. سپس این دنباله از بردارها به یک انکودر استاندارد ترنسفورمر داده می‌شود.
	مزیت اصلی \lr{ViT} نسبت به \lr{CNN} این است که دید سراسری \lr{(Global Context)} را از همان لایه‌های ابتدایی دارد، در حالی که \lr{CNN}ها دید محلی \lr{(Local Receptive Field)} دارند.
	
	\subsection*{۲-۵ مدل‌های چندوجهی \lr{(Multimodal Models)}}
	برای ایجاد یک موتور جستجوی هوشمند که بتواند ارتباط بین «متن» و «تصویر» را درک کند، نیاز به مدل‌های چندوجهی داریم.
	
	\subsubsection*{۲-۵-۱ مدل \lr{CLIP} \lr{(Contrastive Language-Image Pre-training)}}
	مدل \lr{CLIP} که توسط \lr{OpenAI} معرفی شد، یک تحول بزرگ بود. این مدل دو انکودر جداگانه دارد: یکی برای متن و یکی برای تصویر. هدف آموزش این مدل، نزدیک کردن بردار تصویر و بردار متنِ توصیف‌کننده آن در فضای برداری است \lr{(Contrastive Learning)}.
	
	\subsubsection*{۲-۵-۲ مدل \lr{Google SigLIP} (انتخاب این پژوهش)}
	مدل \lr{SigLIP} \lr{(Sigmoid Loss for Language Image Pre-training)} نسخه بهبودیافته \lr{CLIP} است که در این پروژه استفاده شده است.
	
	\textbf{مشکل CLIP:} مدل \lr{CLIP} از تابع زیان \lr{Softmax} استفاده می‌کند که نیاز دارد برای محاسبه احتمالات، تمامی جفت‌های منفی را در مخرج کسر محاسبه کند. این کار در مقیاس‌های بزرگ \lr{(Batch Size بزرگ)} حافظه زیادی مصرف می‌کند.
	
	\textbf{نوآوری SigLIP:} این مدل تابع زیان را به \lr{Sigmoid} تغییر داده است. این تغییر اجازه می‌دهد تا احتمال تطابق هر جفت تصویر-متن به صورت مستقل محاسبه شود.
	
	
	% --- تغییر شماره ۲: استفاده از [H] ---
	\begin{figure}[H]
		\centering
		% فایل تصویر خود را جایگزین کنید
		\includegraphics[width=0.7\textwidth]{images/softmax_sigmoid.png}
		\caption{تفاوت تابع زیان Softmax و Sigmoid}
	\end{figure}
	
	\textbf{مزیت:} \lr{SigLIP} در ابعاد مدل یکسان، دقت بالاتری نسبت به \lr{CLIP} دارد و کارایی آن در استنتاج \lr{(Inference)} بهتر است. ابعاد خروجی مدل \lr{So400m} استفاده شده در این پروژه ۱۱۵۲ می‌باشد که غنای اطلاعاتی بالایی دارد.
	
	\subsection*{۲-۶ مدل‌های تولید متن \lr{(Image Captioning)}}
	برای قابلیت تولید کپشن خودکار، از مدل‌های \lr{Image-to-Text} استفاده شده است.
	
	\subsubsection*{۲-۶-۱ مدل \lr{BLIP}}
	مدل \lr{BLIP} \lr{(Bootstrapping Language-Image Pre-training)} یک معماری یکپارچه برای درک و تولید زبان-تصویر است. این مدل علاوه بر انکودر تصویر و متن، یک دکودر متن \lr{(Text Decoder)} نیز دارد که می‌تواند بر اساس ویژگی‌های استخراج شده از تصویر، جملات معناداری را تولید کند. استفاده از این مدل در پروژه، امکان جستجو بر اساس مفاهیم ریزدانه \lr{(Fine-grained)} را که ممکن است در امبدینگ کلی تصویر گم شوند، فراهم می‌کند.
	
	\subsection*{۲-۷ پایگاه‌های داده برداری \lr{(Vector Databases)}}
	با تولید میلیون‌ها بردار، ذخیره‌سازی و جستجو در آن‌ها به یک چالش تبدیل می‌شود. دیتابیس‌های سنتی \lr{(RDBMS)} برای جستجوی دقیق \lr{(Exact Match)} طراحی شده‌اند و نمی‌توانند جستجوی شباهت \lr{(Similarity Search)} را بهینه انجام دهند.
	
	\subsubsection*{۲-۷-۱ چالش جستجوی نزدیک‌ترین همسایه \lr{(KNN)}}
	مسئله اصلی یافتن $K$ برداری است که کمترین فاصله را با بردار کوئری دارند. روش جستجوی خطی \lr{(Brute-force)} دارای پیچیدگی زمانی $O(N)$ است که برای دیتاست‌های بزرگ بسیار کند است.
	
	\subsubsection*{۲-۷-۲ جستجوی تقریبی \lr{(ANN - Approximate Nearest Neighbors)}}
	برای حل مشکل سرعت، دیتابیس‌های برداری از الگوریتم‌های \lr{ANN} استفاده می‌کنند. این الگوریتم‌ها با فدا کردن مقدار ناچیزی از دقت (مثلاً ۹۹٪ دقت به جای ۱۰۰٪)، سرعت جستجو را هزاران برابر می‌کنند.
	
	\subsubsection*{۲-۷-۳ الگوریتم \lr{HNSW}}
	الگوریتم \lr{HNSW} \lr{(Hierarchical Navigable Small World)} که در دیتابیس \lr{Milvus} استفاده شده، یکی از سریع‌ترین و دقیق‌ترین الگوریتم‌های ایندکس‌گذاری است.
	این الگوریتم ساختاری شبیه به گراف چندلایه ایجاد می‌کند:
	\begin{itemize}
		\item \textbf{لایه بالا:} حاوی تعداد کمی از نقاط داده است (مانند بزرگراه‌ها برای حرکت سریع).
		\item \textbf{لایه‌های پایین:} تراکم نقاط بیشتر می‌شود (مانند خیابان‌های فرعی برای رسیدن به مقصد دقیق).
	\end{itemize}
	جستجو از لایه بالا شروع شده و به صورت حریصانه \lr{(Greedy)} به سمت نزدیک‌ترین گره حرکت می‌کند و سپس به لایه‌های پایین‌تر می‌رود. این ساختار پیچیدگی جستجو را به $O(\log N)$ کاهش می‌دهد.
	
	
	% --- تغییر شماره ۲: استفاده از [H] ---
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{images/hsnw.png}
		\caption{ساختار سلسله‌مراتب گراف در الگوریتم}
	\end{figure}
	
	
	\subsubsection*{۲-۷-۴ متریک‌های شباهت \lr{(Similarity Metrics)}}
	در این پروژه از متریک \lr{Cosine Similarity} (شباهت کسینوسی) استفاده شده است. این متریک زاویه بین دو بردار را اندازه می‌گیرد و به طول بردار \lr{(Magnitude)} حساس نیست.
	
	فرمول شباهت کسینوسی:
	$$Similarity(A, B) = \frac{A \cdot B}{||A|| \times ||B||}$$
	
	از آنجایی که بردارهای خروجی مدل \lr{SigLIP} نرمالایز می‌شوند، ضرب داخلی \lr{(Dot Product)} و شباهت کسینوسی عملکرد یکسانی دارند.
	
	
	
	
	% ------------------------------------
	\section*{۳. معماری و طراحی سیستم}
	% ------------------------------------
	
	\subsection*{۳-۱ مقدمه}
	پس از بررسی مبانی نظری در فصل پیشین، در این فصل به تشریح معماری نرم‌افزار، طراحی اجزاء و جریان داده‌ها در سامانه موتور جستجوی هوشمند می‌پردازیم. معماری این سیستم بر پایه رویکرد میکروسرویس \lr{(Microservices)} و با استفاده از کانتینرهای داکر طراحی شده است تا پایداری، مقیاس‌پذیری و نگهداری آسان را تضمین نماید. طراحی سیستم به سه لایه اصلی تقسیم می‌شود: لایه رابط کاربری \lr{(Presentation Layer)}، لایه منطق و پردازش \lr{(Logic/Processing Layer)} و لایه داده \lr{(Data Persistence Layer)}.
	
	\subsection*{۳-۲ معماری سطح کلان \lr{(High-Level Architecture)}}
	معماری سیستم به گونه‌ای طراحی شده است که اجزای پردازشی \lr{(AI Models)} از اجزای ذخیره‌سازی \lr{(Database)} مستقل باشند. این استقلال باعث می‌شود در صورت نیاز به تغییر مدل هوش مصنوعی یا ارتقای دیتابیس، سایر بخش‌ها کمترین تأثیر را بپذیرند.
	
	دیاگرام کلی سیستم شامل چهار بخش اصلی است:
	\begin{itemize}
		\item \textbf{کلاینت \lr{(Client)}:} رابط کاربری وب که با \lr{Streamlit} پیاده‌سازی شده است.
		\item \textbf{موتور هوش مصنوعی \lr{(AI Engine)}:} مسئول تبدیل داده‌های خام (متن/تصویر) به بردارهای ریاضی و تولید کپشن.
		\item \textbf{مدیریت داده \lr{(Data Manager)}:} واسط بین برنامه و دیتابیس \lr{Milvus}.
		\item \textbf{زیرساخت ذخیره‌سازی \lr{(Storage Infra)}:} شامل \lr{Milvus} (برای بردارها) و \lr{File System} (برای تصاویر خام).
	\end{itemize}
	
	% --- تغییر شماره ۲: استفاده از [H] ---
	\begin{figure}[H]
		\centering
		% نام فایل دیاگرام معماری خود را جایگزین کنید
		\includegraphics[width=0.9\textwidth]{images/high_level_new.png}
		
		\caption{دیاگرام معماری کلان سیستم}
	\end{figure}
	
	\subsection*{۳-۳ طراحی اجزای سیستم \lr{(Component Design)}}
	
	\subsubsection*{۳-۳-۱ لایه رابط کاربری \lr{(Frontend)}}
	این لایه نقطه تعامل کاربر با سیستم است. با توجه به نیاز به تعامل پذیری بالا \lr{(Interactive UI)} برای کارهایی نظیر برش تصاویر \lr{(Cropping)} و مشاهده بلادرنگ نتایج، از فریم‌ورک \lr{Streamlit} استفاده شده است.
	این لایه شامل ماژول‌های زیر است:
	\begin{itemize}
		\item \textbf{ماژول آپلود \lr{(Upload Module)}:} مدیریت دریافت فایل‌ها، بررسی فرمت \lr{(JPG/PNG)} و ذخیره‌سازی موقت.
		\item \textbf{ماژول برش \lr{(Cropper Widget)}:} ابزار گرافیکی برای انتخاب ناحیه خاصی از تصویر \lr{(Region of Interest - ROI)}.
		\item \textbf{ماژول نمایش نتایج \lr{(Result Renderer)}:} مسئول نمایش تصاویر یافت شده به صورت شبکه \lr{(Grid)} و نمایش امتیاز شباهت با کدگذاری رنگی \lr{(Color-coded Similarity Score)}.
	\end{itemize}
	
	\subsubsection*{۳-۳-۲ لایه منطق هوش مصنوعی \lr{(AI Core)}}
	این لایه «مغز متفکر» سیستم است و در فایل \lr{\texttt{core/ai\_engine.py}} پیاده‌سازی شده است. وظایف این لایه عبارتند از:
	\begin{itemize}
		\item \textbf{مدیر مدل‌ها \lr{(Model Loader)}:} با استفاده از دیزاین پترن \lr{Singleton} (از طریق \lr{\texttt{st.cache\_resource}})، مدل‌های سنگین \lr{SigLIP} و \lr{BLIP} را تنها یک‌بار در حافظه \lr{GPU} بارگذاری می‌کند تا از سربار \lr{(Overhead)} جلوگیری شود.
		\item \textbf{تعبیه برداری \lr{(Embedding Generator)}:} تصویر یا متن ورودی را دریافت کرده، پیش‌پردازش‌های لازم (تغییر اندازه به ۳۸۴×۳۸۴ پیکسل و نرمال‌سازی) را انجام داده و یک بردار با ابعاد ۱۱۵۲ تولید می‌کند.
		\item \textbf{تولید متن \lr{(Caption Generator)}:} تصویر را به مدل \lr{BLIP} داده و توصیف متنی آن را دریافت می‌کند.
	\end{itemize}
	
	\subsubsection*{۳-۳-۳ لایه مدیریت داده \lr{(Database Abstraction Layer)}}
	برای جلوگیری از وابستگی مستقیم کدهای رابط کاربری به دستورات دیتابیس، یک لایه انتزاعی در فایل \lr{\texttt{core/db\_manager.py}} طراحی شده است. این لایه تمام عملیات \lr{CRUD} (ایجاد، خواندن، بروزرسانی، حذف) را کپسوله می‌کند.
	ویژگی‌های کلیدی این لایه:
	\begin{itemize}
		\item مدیریت اتصال \lr{(Connection Handling)} به کانتینر \lr{Milvus}.
		\item مدیریت خطا \lr{(Error Handling)} در صورت قطع شبکه.
		\item پیاده‌سازی منطق جستجوی ترکیبی (ترکیب فیلتر اسکالر و برداری).
	\end{itemize}
	
	\subsection*{۳-۴ طراحی جریان داده \lr{(Data Flow Design)}}
	درک نحوه حرکت داده‌ها در سیستم برای تحلیل کارایی ضروری است. در اینجا دو سناریوی اصلی سیستم را بررسی می‌کنیم.
	
	\subsubsection*{۳-۴-۱ سناریوی درج داده \lr{(Indexing Pipeline)}}
	این فرآیند زمانی رخ می‌دهد که کاربر یک تصویر جدید را به سیستم اضافه می‌کند. مراحل به شرح زیر است:
	\begin{enumerate}
		\item \textbf{دریافت:} تصویر توسط کاربر آپلود می‌شود.
		\item \textbf{ذخیره فیزیکی:} تصویر اصلی در مسیر \lr{\texttt{/home/jovyan/...}} روی سرور ذخیره می‌شود.
		\item \textbf{پردازش AI:}
		\begin{itemize}
			\item تصویر به مدل \lr{SigLIP} ارسال شده $\leftarrow$ بردار ویژگی \lr{(Feature Vector)} استخراج می‌شود.
			\item (اختیاری) تصویر به مدل \lr{BLIP} ارسال شده $\leftarrow$ کپشن تولید می‌شود.
		\end{itemize}
		\item \textbf{بسته‌بندی داده:} بردار، مسیر فایل و کپشن در یک دیکشنری بسته‌بندی می‌شوند.
		\item \textbf{درج در پایگاه داده:} داده‌ها به \lr{Milvus} ارسال شده و در حافظه موقت \lr{(Buffer)} قرار می‌گیرند تا ایندکس شوند.
	\end{enumerate}
	
	% --- تغییر شماره ۲: استفاده از [H] ---
	\begin{figure}[H]
		\centering
		
		\includegraphics[width=1\textwidth]{images/buffer.png}
		\caption{فلوچارت فرآیند درج تصویر}
	\end{figure}
	
	\subsubsection*{۳-۴-۲ سناریوی جستجو \lr{(Search Pipeline)}}
	این فرآیند پیچیده‌تر است و شامل مراحل زیر می‌باشد:
	\begin{enumerate}
		\item \textbf{دریافت کوئری:} کاربر متن یا تصویری را وارد می‌کند.
		\item \textbf{تبدیل به بردار:} کوئری به بردار عددی \lr{(Query Vector)} تبدیل می‌شود.
		\item \textbf{اعمال فیلتر (در حالت Hybrid):} اگر کاربر فیلتری (مثلاً نام فایل) تعیین کرده باشد، ابتدا \lr{Milvus} فضای جستجو را محدود می‌کند \lr{(Pre-filtering)}.
		\item \textbf{جستجوی نزدیک‌ترین همسایه \lr{(ANN Search)}:} الگوریتم \lr{HNSW} بردار کوئری را با بردارهای موجود مقایسه کرده و $K$ مورد شبیه‌ترین را می‌یابد.
		\item \textbf{بازیابی اطلاعات:} مسیر فایل \lr{(Path)} و کپشن مربوط به بردارهای یافت شده بازگردانده می‌شود.
		\item \textbf{اعتبارسنجی فایل:} سیستم چک می‌کند آیا فایل فیزیکی روی دیسک موجود است یا خیر.
		\item \textbf{نمایش:} نتایج معتبر به کاربر نمایش داده می‌شوند.
	\end{enumerate}
	
	% --- تغییر شماره ۲: استفاده از [H] ---
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{images/digram.png}
		\caption{دیاگرام توالی عملیات جستجو}
	\end{figure}
	
	\subsection*{۳-۵ طراحی پایگاه داده \lr{(Database Schema)}}
	دیتابیس \lr{Milvus} بر خلاف دیتابیس‌های رابطه‌ای، دارای جدول \lr{(Table)} نیست و از مفهوم \lr{Collection} استفاده می‌کند. طراحی اسکیمای \lr{(Schema)} کالکشن \lr{\texttt{siglip\_gallery\_v3}} در جدول زیر آمده است:
	
	
	
	\begin{table}[H]
		\centering
		\caption{مشخصات اسکیمای پایگاه داده}
		\vspace{0.5em}
		\begin{tabular}{|c|c|p{6cm}|c|}
			\hline
			\textbf{نام فیلد} & \textbf{نوع داده} \lr{(Data Type)} & \textbf{توضیحات} & \textbf{ویژگی‌ها} \\
			\hline
			\lr{id} & \lr{Int64} & شناسه یکتای تصویر & \lr{Primary Key, Auto-ID} \\
			\hline
			\lr{vector} & \lr{FloatVector} & بردار ویژگی استخراج شده & \lr{Dim: 1152} \\
			\hline
			\lr{path} & \lr{VarChar (1024)} & آدرس ذخیره‌سازی فایل روی دیسک & - \\
			\hline
			\lr{caption} & \lr{VarChar (2048)} & توضیحات متنی (تولید شده یا دستی) & - \\
			\hline
		\end{tabular}
	\end{table}
	
	\subsubsection*{۳-۵-۱ استراتژی ایندکس‌گذاری \lr{(Indexing Strategy)}}
	برای بهینه‌سازی سرعت جستجو، از ایندکس \lr{HNSW} با پارامترهای زیر استفاده شده است:
	\begin{itemize}
		\item \textbf{$M = 16$:} تعداد اتصالات هر گره در گراف. مقدار بالاتر دقت را افزایش و سرعت ساخت ایندکس را کاهش می‌دهد.
		\item \textbf{$efConstruction = 200$:} عمق جستجو در هنگام ساخت گراف.
		\item \textbf{Metric Type = COSINE:} از آنجایی که بردارهای \lr{SigLIP} نرمالایز شده‌اند، فاصله کسینوسی بهترین معیار برای سنجش شباهت معنایی است.
	\end{itemize}
	
	\subsection*{۳-۶ طراحی زیرساخت و استقرار \lr{(Infrastructure \& Deployment)}}
	تمامی سرویس‌ها بر بستر \lr{Docker} اجرا می‌شوند. فایل \lr{\texttt{docker-compose.yml}} وظیفه ارکستراسیون \lr{(Orchestration)} سه کانتینر اصلی را بر عهده دارد:
	\begin{itemize}
		\item \textbf{\lr{Milvus Standalone}:} هسته مرکزی دیتابیس.
		\item \textbf{\lr{Etcd}:} مدیریت متادیتا و هماهنگی سرویس‌های داخلی \lr{Milvus}.
		\item \textbf{\lr{MinIO}:} ذخیره‌سازی آبجکت‌ها (لاگ‌ها و فایل‌های ایندکس \lr{Milvus}).
	\end{itemize}
	
	\subsubsection*{۳-۶-۱ پیکربندی شبکه \lr{(Network Configuration)}}
	یکی از چالش‌های طراحی، ارتباط کانتینر اپلیکیشن (که ممکن است جداگانه اجرا شود) با کانتینرهای دیتابیس بود. برای حل این مشکل، تمامی کانتینرها در یک شبکه مشترک \lr{(Bridge Network)} به نام \lr{\texttt{milvus\_default}} قرار گرفتند. این طراحی به کانتینرها اجازه می‌دهد با استفاده از نام سرویس \lr{(Service Discovery)} و بدون نیاز به \lr{IP} ثابت با یکدیگر ارتباط برقرار کنند.
	
	\subsection*{۳-۷ طراحی امنیتی و کارایی \lr{(Security \& Performance)}}
	
	\subsubsection*{۳-۷-۱ ملاحظات کارایی}
	\begin{itemize}
		\item \textbf{کش کردن مدل‌ها:} استفاده از دکوریتور \lr{\texttt{@st.cache\_resource}} برای جلوگیری از لود شدن تکراری مدل‌های سنگین \lr{AI} در هر بار رفرش صفحه.
		\item \textbf{پردازش دسته‌ای \lr{(Batch Processing)}:} در هنگام اینسرت کردن تعداد زیادی عکس \lr{(Batch Folder)}، تصاویر به صورت دسته‌ای پردازش می‌شوند تا بهره‌وری \lr{GPU} افزایش یابد.
	\end{itemize}
	
	\subsubsection*{۳-۷-۲ ملاحظات امنیتی}
	\begin{itemize}
		\item \textbf{ایزولاسیون:} اجرای دیتابیس در کانتینر، دسترسی مستقیم به داده‌ها از خارج سرور را محدود می‌کند.
		\item \textbf{اعتبارسنجی ورودی:} در بخش آپلود فایل، فرمت فایل‌ها چک می‌شود تا از آپلود فایل‌های مخرب جلوگیری شود.
	\end{itemize}
	
	
% ------------------------------------
\section*{۴. پیاده‌سازی و ارزیابی سیستم}
% ------------------------------------

\subsection*{۴-۱ مقدمه}
در این فصل به جزئیات پیاده‌سازی کدهای توسعه‌یافته و چالش‌های فنی حین اجرا می‌پردازیم. پیاده‌سازی این سامانه با زبان برنامه‌نویسی \lr{Python 3.10} انجام شده است. تمرکز اصلی در این بخش بر روی دو ماژول حیاتی سیستم، یعنی «موتور هوش مصنوعی» و «مدیر پایگاه داده» است. در پایان فصل، عملکرد سیستم در سناریوهای واقعی مورد ارزیابی قرار گرفته است.

\subsection*{۴-۲ پیاده‌سازی موتور هوش مصنوعی \lr{(AI Engine Implementation)}}
کلاس \lr{\texttt{AIEngine}} وظیفه تعامل با مدل‌های ترنسفورمر را بر عهده دارد. یکی از نکات کلیدی در پیاده‌سازی این کلاس، مدیریت حافظه \lr{GPU} است. از آنجایی که بارگذاری مدل‌های \lr{SigLIP} و \lr{BLIP} زمان‌بر است و حافظه زیادی اشغال می‌کند، از الگوی \lr{Singleton} و دکوریتور \lr{Caching} فریم‌ورک \lr{Streamlit} استفاده شده است.

\begin{latin}
	\begin{verbatim}
		class AIEngine:
		@staticmethod
		@st.cache_resource
		def load_embedding_model():
		# Load SigLIP Model (So400m)
		model = SigLIP.from_pretrained(config.EMBEDDING_MODEL).to(DEVICE)
		return model
		
		def get_embedding(self, image=None, text=None):
		model, processor = self.load_embedding_model()
		if image:
		inputs = processor(images=image, return_tensors="pt")
		elif text:
		inputs = processor(text=[text], return_tensors="pt")
		
		with torch.no_grad():
		features = model.get_features(**inputs)
		# Normalization (L2 Norm)
		features = features / features.norm(p=2, dim=-1, keepdim=True)
		return features.cpu().numpy()
	\end{verbatim}
\end{latin}

همان‌طور که در کد بالا مشاهده می‌شود، عملیات \lr{Normalization} پس از استخراج ویژگی‌ها انجام می‌شود. این مرحله برای استفاده از متریک \lr{Cosine Similarity} در دیتابیس \lr{Milvus} حیاتی است.

\subsection*{۴-۳ پیاده‌سازی مدیریت دیتابیس \lr{(DB Manager Implementation)}}
کلاس \lr{\texttt{DBManager}} به عنوان یک لایه انتزاعی \lr{(Abstraction Layer)} عمل می‌کند. این کلاس پیچیدگی‌های کار با \lr{SDK} میلووس را مخفی کرده و توابع ساده‌ای مانند \lr{insert} و \lr{search} را در اختیار لایه رابط کاربری قرار می‌دهد.

یکی از چالش‌های پیاده‌سازی در این بخش، مدیریت فیلترهای ترکیبی \lr{(Hybrid Filters)} بود. برای حل این مسئله، تابع جستجو به گونه‌ای طراحی شد که عبارات \lr{SQL-like} را بپذیرد.

\begin{latin}
	\begin{verbatim}
		def search(self, vector, top_k=5, filter_expr=None):
		search_params = {
			"metric_type": "COSINE",
			"params": {"nprobe": 10}
		}
		
		# Execute Search
		res = self.client.search(
		collection_name=config.COLLECTION_NAME,
		data=[vector],
		limit=top_k,
		filter=filter_expr,  # Apply Hybrid Filter (e.g., path like '%.jpg')
		output_fields=["path", "caption"],
		search_params=search_params
		)
		return res[0]
	\end{verbatim}
\end{latin}

\subsection*{۴-۴ چالش‌های فنی و راه‌حل‌ها}

\subsubsection*{۴-۴-۱ چالش ارتباط شبکه در داکر \lr{(Docker Networking)}}
\textbf{مسئله:} در مراحل اولیه توسعه، کانتینر رابط کاربری \lr{(App)} قادر به برقراری ارتباط با کانتینر دیتابیس \lr{(Milvus)} نبود و خطای \lr{Connection Refused} دریافت می‌شد.

\textbf{تحلیل:} بررسی‌ها نشان داد که هر فایل \lr{docker-compose} یک شبکه ایزوله ایجاد می‌کند. کانتینر \lr{Jupyter/App} به صورت دستی اجرا شده بود و خارج از شبکه \lr{bridge} دیتابیس قرار داشت.

\textbf{راه‌حل:} برای حل این مشکل، از قابلیت \lr{Docker Network Connect} استفاده شد تا کانتینر اپلیکیشن به شبکه دیتابیس ملحق شود:
\begin{latin}
	\begin{verbatim}
		sudo docker network connect milvus_default <container_id>
	\end{verbatim}
\end{latin}
همچنین در فایل تنظیمات، آدرس اتصال از \lr{localhost} به نام سرویس داخلی \lr{milvus-standalone} تغییر یافت.

\subsection*{۴-۵ ارزیابی عملکرد سیستم \lr{(System Evaluation)}}
برای ارزیابی کارایی سیستم، سناریوهای مختلف جستجو روی مجموعه داده \lr{Flickr30k} آزمایش شد.

\subsubsection*{۴-۵-۱ سناریوی جستجوی متن-به-عکس}
در این آزمایش، کوئری متنی «یک ماشین قرمز در خیابان» \lr{(A red car in the street)} به سیستم داده شد.
\textbf{نتیجه:} سیستم توانست تصاویری را بازیابی کند که اگرچه در نام فایل آن‌ها کلمه \lr{car} وجود نداشت، اما محتوای بصری دقیقاً مطابق با درخواست بود. زمان پاسخگویی برای جستجو در میان ۱۰,۰۰۰ تصویر زیر ۸۰ میلی‌ثانیه ثبت شد.

\subsubsection*{۴-۵-۲ سناریوی جستجوی ناحیه‌ای \lr{(Crop \& Search)}}
در این سناریو، تصویری شامل چندین المان (انسان، دوچرخه، درخت) بارگذاری شد. با استفاده از ابزار \lr{Cropper}، تنها ناحیه مربوط به «دوچرخه» انتخاب و جستجو شد.
\textbf{نتیجه:} نتایج جستجو به جای نمایش تصاویر شبیه به کل صحنه، دقیقاً تصاویری از انواع دوچرخه‌ها را نمایش دادند که نشان‌دهنده عملکرد صحیح مکانیزم توجه \lr{(Attention Mechanism)} مدل در ناحیه برش‌خورده است.

\subsubsection*{۴-۵-۳ سناریوی پاک‌سازی داده‌های تکراری}
برای تست ماژول \lr{Deduplication}، تعداد ۵۰ تصویر تکراری با نام‌های مختلف به دیتابیس تزریق شد.\newline


\textbf{نتیجه:} سیستم با محاسبه فاصله برداری (که برای تصاویر یکسان برابر صفر است)، موفق به شناسایی تمامی ۵۰ مورد تکراری شد و با تایید کاربر، آن‌ها را هم از دیتابیس و هم از دیسک حذف نمود.

\subsection*{۴-۶ نتیجه‌گیری و کارهای آینده}
این پژوهش نشان داد که استفاده از دیتابیس‌های برداری نظیر \lr{Milvus} در کنار مدل‌های قدرتمند \lr{SigLIP}، می‌تواند راهکاری کارآمد برای چالش مدیریت داده‌های تصویری در سازمان‌ها باشد. سیستم پیاده‌سازی شده با موفقیت توانست شکاف معنایی بین متن و تصویر را پر کرده و امکاناتی فراتر از جستجوی سنتی ارائه دهد.
در ادامه ابزار هایی مانند \lr{ocr} و شناسایی متن از تصویر و ...  هم به این پروژه اضافه خواهد شد.
\end{document}