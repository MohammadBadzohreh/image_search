# core/ai_engine.py
import streamlit as st
from transformers import (
    AutoProcessor, SiglipModel, AutoModel, 
    BlipProcessor, BlipForConditionalGeneration,
    CLIPModel, CLIPProcessor
)
from PIL import Image
import torch
import config
# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ CLIPA
import open_clip 

class AIEngine:
    
    @staticmethod
    @st.cache_resource
    def load_embedding_model(model_key):
        cfg = config.MODELS_CONFIG[model_key]
        print(f"ðŸ”„ Loading {model_key} ({cfg['model_id']}) on {config.DEVICE}...")
        
        # 1. SigLIP
        if cfg["type"] == "siglip":
            model = SiglipModel.from_pretrained(cfg["model_id"]).to(config.DEVICE)
            processor = AutoProcessor.from_pretrained(cfg["model_id"])
            return model, processor, "siglip"
            
        # 2. Jina CLIP
        elif cfg["type"] == "jina":
            model = AutoModel.from_pretrained(cfg["model_id"], trust_remote_code=True).to(config.DEVICE)
            return model, None, "jina"
            
        # 3. OpenCLIP (CLIPA)
        elif cfg["type"] == "open_clip":
            model, _, preprocess = open_clip.create_model_and_transforms(cfg["model_id"], device=config.DEVICE)
            tokenizer = open_clip.get_tokenizer(cfg["model_id"])
            return model, (preprocess, tokenizer), "open_clip"
            
        # 4. Llama Nemo (Multimodal) ðŸ‘‡
        elif cfg["type"] == "llama_nemo":
            # Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø³Ù†Ú¯ÛŒÙ† Ø§Ø³Øª (3B)ØŒ Ø§Ú¯Ø± GPU Ø¯Ø§Ø±ÛŒØ¯ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¨Ø§ float16 Ù„ÙˆØ¯ Ø´ÙˆØ¯
            dtype = torch.float16 if config.DEVICE == "cuda" else torch.float32
            model = AutoModel.from_pretrained(
                cfg["model_id"], 
                trust_remote_code=True, 
                torch_dtype=torch.float32  # ðŸ‘ˆ Ø§ÛŒÙ† Ø®Ø· Ù…Ø´Ú©Ù„ Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                # torch_dtype=dtype
            ).to(config.DEVICE)
            # Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø®ÙˆØ¯Ø´ Ù¾Ø±ÙˆØ³Ø³ÙˆØ± Ø¯Ø§Ø®Ù„ÛŒ Ø¯Ø§Ø±Ø¯
            return model, None, "llama_nemo"

    def get_embedding(self, model_key, image=None, text=None):
        loaded_data = self.load_embedding_model(model_key)
        
        if len(loaded_data) == 3:
            model, processor, model_type = loaded_data
        else:
            model, processor, model_type = loaded_data[0], None, "jina"

        vector = None
        
        with torch.no_grad():
            
            # --- Ù…Ù†Ø·Ù‚ Llama Nemo (Multimodal) ðŸ‘‡ ---
            if model_type == "llama_nemo":
                embeddings = None
                
                if image:
                    # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ±
                    if isinstance(image, str): 
                        pil_image = Image.open(image).convert("RGB")
                    else: 
                        pil_image = image.convert("RGB")
                    
                    # Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù…ØªØ¯ Ø§Ø®ØªØµØ§ØµÛŒ forward_passages Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø¹Ú©Ø³ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
                    # Ø®Ø±ÙˆØ¬ÛŒ: [batch, num_tokens, dim]
                    output = model.forward_passages([pil_image], batch_size=1)
                    embeddings = output
                    
                elif text:
                    # Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù…ØªØ¯ Ø§Ø®ØªØµØ§ØµÛŒ forward_queries Ø¯Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†
                    output = model.forward_queries([text], batch_size=1)
                    embeddings = output

                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªÚ©â€ŒØ¨Ø±Ø¯Ø§Ø± (Mean Pooling)
                # Ú†ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ ColBERT Ú†Ù†Ø¯ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø³ØªØŒ Ø¨Ø±Ø§ÛŒ Milvus Ø¨Ø§ÛŒØ¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ú¯ÛŒØ±ÛŒÙ…
                if embeddings is not None:
                    # embeddings shape: [1, seq_len, 3072]
                    pooled = embeddings.mean(dim=1) # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§
                    
                    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (L2 Norm)
                    pooled = pooled / pooled.norm(p=2, dim=-1, keepdim=True)
                    
                    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ float32 (Ø§Ú¯Ø± Ù…Ø¯Ù„ fp16 Ø¨Ø§Ø´Ø¯ØŒ Ù†Ø§Ù…Ù¾Ø§ÛŒ Ø¨Ø§ÛŒØ¯ 32 Ø¨Ø§Ø´Ø¯)
                    vector = pooled[0].float().cpu().numpy()

            # --- Ù…Ù†Ø·Ù‚ SigLIP ---
            elif model_type == "siglip":
                if image:
                    if isinstance(image, str): image = Image.open(image).convert("RGB")
                    inputs = processor(images=image, return_tensors="pt").to(config.DEVICE)
                    features = model.get_image_features(**inputs)
                elif text:
                    inputs = processor(text=[text], return_tensors="pt", padding="max_length", max_length=64).to(config.DEVICE)
                    features = model.get_text_features(**inputs)
                
                features = features / features.norm(p=2, dim=-1, keepdim=True)
                vector = features[0].cpu().numpy()

            # --- Ù…Ù†Ø·Ù‚ Jina CLIP ---
            elif model_type == "jina":
                if image:
                    if isinstance(image, str): image = Image.open(image).convert("RGB")
                    vector = model.encode_image(image) 
                elif text:
                    vector = model.encode_text(text)
                
                if isinstance(vector, torch.Tensor): vector = vector.cpu().numpy()
                if vector.ndim > 1: vector = vector[0]

            # --- Ù…Ù†Ø·Ù‚ OpenCLIP (CLIPA) ---
            elif model_type == "open_clip":
                preprocess, tokenizer = processor
                if image:
                    if isinstance(image, str): image = Image.open(image).convert("RGB")
                    image_tensor = preprocess(image).unsqueeze(0).to(config.DEVICE)
                    features = model.encode_image(image_tensor)
                elif text:
                    text_tokens = tokenizer([text]).to(config.DEVICE)
                    features = model.encode_text(text_tokens)
                
                features = features / features.norm(p=2, dim=-1, keepdim=True)
                vector = features[0].cpu().numpy()

        return vector

    # --- BLIP (Caption) ---
    @staticmethod
    @st.cache_resource
    def load_caption_model():
        processor = BlipProcessor.from_pretrained(config.CAPTION_MODEL)
        model = BlipForConditionalGeneration.from_pretrained(config.CAPTION_MODEL).to(config.DEVICE)
        return model, processor

    def generate_caption(self, image_path):
        model, processor = self.load_caption_model()
        try:
            raw_image = Image.open(image_path).convert('RGB')
            inputs = processor(raw_image, return_tensors="pt").to(config.DEVICE)
            with torch.no_grad():
                out = model.generate(**inputs, max_new_tokens=50)
            return processor.decode(out[0], skip_special_tokens=True)
        except Exception as e:
            return "error in ai engine"